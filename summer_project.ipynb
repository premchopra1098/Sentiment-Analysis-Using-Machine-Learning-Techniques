{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading test and train data\n",
    "train  = pd.read_csv('C:/Users/dell/Downloads/train_E6oV3lV.csv') \n",
    "test = pd.read_csv('C:/Users/dell/Downloads/test_tweets_anuFYb8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading test and train data\n",
    "#use forward slash instead of backward to read data from desktop\n",
    "train  = pd.read_csv('C:/Users/dell/Downloads/train_E6oV3lV.csv') \n",
    "test = pd.read_csv('C:/Users/dell/Downloads/test_tweets_anuFYb8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      0   \n",
       "5   6      0   \n",
       "6   7      0   \n",
       "7   8      0   \n",
       "8   9      0   \n",
       "9  10      0   \n",
       "\n",
       "                                                                                                                                             tweet  \n",
       "0                                            @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1                       @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                                                              bihday your majesty  \n",
       "3                                                           #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                                                           factsguide: society now    #motivation  \n",
       "5                             [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo    \n",
       "6                                                                        @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦  \n",
       "7  the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl  \n",
       "8                                                          we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦   \n",
       "9                                                                                                @user @user welcome here !  i'm   it's so #gr8 !   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['label']==0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>@user lets fight against  #love #peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>@user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>how the #altright uses  &amp;amp; insecurity to lure men into #whitesupremacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>@user i'm not interested in a #linguistics that doesn't address #race &amp;amp; . racism is about #power. #raciolinguistics bringsâ¦</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label  \\\n",
       "13    14      1   \n",
       "14    15      1   \n",
       "17    18      1   \n",
       "23    24      1   \n",
       "34    35      1   \n",
       "56    57      1   \n",
       "68    69      1   \n",
       "77    78      1   \n",
       "82    83      1   \n",
       "111  112      1   \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "13                                                          @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot    \n",
       "14                               no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins  \n",
       "17                                                                                                              retweet if you agree!   \n",
       "23                                                                                     @user @user lumpy says i am a . prove it lumpy.  \n",
       "34                            it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia   \n",
       "56                                                                                             @user lets fight against  #love #peace   \n",
       "68                      ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness    \n",
       "77                                             @user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦  \n",
       "82                                                       how the #altright uses  &amp; insecurity to lure men into #whitesupremacy      \n",
       "111  @user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics bringsâ¦  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['label']==1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17197, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_train = train['tweet'].str.len() \n",
    "length_test = test['tweet'].str.len() \n",
    "plt.hist(length_train, bins=20, label=\"train_tweets\") \n",
    "plt.hist(length_test, bins=20, label=\"test_tweets\") \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine train and test data \n",
    "combi = train.append(test,ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given below is a user-defined function to remove unwanted text patterns from the tweets.\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                                         tidy_tweet  \n",
       "0                   when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1    thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                               bihday your majesty  \n",
       "3                            #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                            factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Twitter Handles (@user)\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction    #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks for #lyft credit i can t use cause they don t offer wheelchair vans in pdx     #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model   i love u take with u all the time in ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide  society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n",
       "      <td>huge fan fare and big talking before they leave  chaos and pay disputes when they get there  #allshowandnogo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>\n",
       "      <td>camping tomorrow        danny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "      <td>the next school year is the year for exams      can t think about that      #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>\n",
       "      <td>we won    love the land    #allin #cavs #champions #cleveland #clevelandcavaliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>\n",
       "      <td>welcome here    i m   it s so #gr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "5   6    0.0   \n",
       "6   7    0.0   \n",
       "7   8    0.0   \n",
       "8   9    0.0   \n",
       "9  10    0.0   \n",
       "\n",
       "                                                                                                                                             tweet  \\\n",
       "0                                            @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1                       @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                                              bihday your majesty   \n",
       "3                                                           #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                                           factsguide: society now    #motivation   \n",
       "5                             [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo     \n",
       "6                                                                        @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦   \n",
       "7  the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl   \n",
       "8                                                          we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦    \n",
       "9                                                                                                @user @user welcome here !  i'm   it's so #gr8 !    \n",
       "\n",
       "                                                                                                                                        tidy_tweet  \n",
       "0                                                  when a father is dysfunctional and is so selfish he drags his kids into his dysfunction    #run  \n",
       "1                                   thanks for #lyft credit i can t use cause they don t offer wheelchair vans in pdx     #disapointed #getthanked  \n",
       "2                                                                                                                              bihday your majesty  \n",
       "3                                                           #model   i love u take with u all the time in ur                                        \n",
       "4                                                                                                           factsguide  society now    #motivation  \n",
       "5                                   huge fan fare and big talking before they leave  chaos and pay disputes when they get there  #allshowandnogo    \n",
       "6                                                                                                                 camping tomorrow        danny     \n",
       "7  the next school year is the year for exams      can t think about that      #school #exams   #hate #imagine #actorslife #revolutionschool #girl  \n",
       "8                                                          we won    love the land    #allin #cavs #champions #cleveland #clevelandcavaliers        \n",
       "9                                                                                                            welcome here    i m   it s so #gr      "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Punctuations, Numbers, and Special Characters\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \") \n",
    "combi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when father dysfunctional selfish drags kids into dysfunction #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks #lyft credit cause they offer wheelchair vans #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model love take with time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                      tidy_tweet  \n",
       "0             when father dysfunctional selfish drags kids into dysfunction #run  \n",
       "1  thanks #lyft credit cause they offer wheelchair vans #disapointed #getthanked  \n",
       "2                                                            bihday your majesty  \n",
       "3                                                     #model love take with time  \n",
       "4                                                 factsguide society #motivation  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing sort words\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [when, father, dysfunctional, selfish, drags, kids, into, dysfunction, #run]\n",
       "1    [thanks, #lyft, credit, cause, they, offer, wheelchair, vans, #disapointed, #getthanked]\n",
       "2                                                                     [bihday, your, majesty]\n",
       "3                                                            [#model, love, take, with, time]\n",
       "4                                                          [factsguide, society, #motivation]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "tokenized_tweet=combi['tidy_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import poterstemmer from nltk library\n",
    "from __future__ import print_function\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=0.90,ngram_range=(1,2), min_df=2, max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90,ngram_range=(1,2), min_df=2, max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6510028, 7536020)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training a Word2Vec model on our corpus\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#avocado', 0.5680983662605286),\n",
       " ('#biall', 0.5515506267547607),\n",
       " ('cookout', 0.5508288145065308),\n",
       " ('spaghetti', 0.5503631830215454),\n",
       " ('melani', 0.5367459654808044),\n",
       " ('#cellar', 0.5350143909454346),\n",
       " ('fav', 0.5342785120010376),\n",
       " ('spinach', 0.5333420634269714),\n",
       " ('#bihdaydinn', 0.5331523418426514),\n",
       " ('noodl', 0.5329619646072388)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing our model\n",
    "model_w2v.wv.most_similar(positive=\"dinner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.20504656e-01,  2.35428244e-01, -6.65115535e-01,  5.31672776e-01,\n",
       "       -8.53725433e-01,  3.18220228e-01, -2.01517195e-01,  4.08881694e-01,\n",
       "       -4.56183612e-01, -2.22480863e-01,  4.46627259e-01,  4.86405015e-01,\n",
       "        6.66425601e-02, -8.48116636e-01, -4.51006651e-01,  5.08116722e-01,\n",
       "        7.70817325e-02, -3.60659212e-02, -1.35965347e-01,  2.65888143e-02,\n",
       "       -5.52225590e-01,  4.48081642e-01, -5.05010784e-01, -3.32603008e-01,\n",
       "       -3.97511482e-01, -2.44809717e-01,  1.23941377e-01,  5.02361655e-01,\n",
       "        3.51046830e-01,  9.04936790e-02, -5.27797267e-03,  1.97484761e-01,\n",
       "       -1.13055430e-01,  9.60133225e-02, -8.89795795e-02,  1.00747570e-01,\n",
       "        5.37672155e-02,  2.49574050e-01,  1.49701059e-01,  1.69084325e-01,\n",
       "        2.59170771e-01,  3.93858790e-01,  3.65755260e-01,  2.10033953e-02,\n",
       "        1.08677042e+00, -7.02806056e-01, -1.26961514e-01, -7.59154499e-01,\n",
       "        1.37438595e-01,  1.44581990e-02,  8.53074074e-01, -9.43284556e-02,\n",
       "        2.64084369e-01,  9.41812992e-01, -2.85672188e-01,  6.82168305e-01,\n",
       "       -2.77064621e-01, -1.28165111e-01,  2.03455716e-01, -2.36615837e-01,\n",
       "       -4.16026145e-01,  4.67688352e-01,  1.41553596e-01,  7.83740520e-01,\n",
       "       -2.77270377e-01,  4.05203104e-01, -5.66425025e-01, -1.84624404e-01,\n",
       "        6.92742944e-01,  2.29654431e-01, -2.26647705e-01, -1.22384477e+00,\n",
       "        4.07510191e-01,  3.92261535e-01,  1.04720965e-01, -3.89131814e-01,\n",
       "        5.64754665e-01,  2.82713592e-01,  9.94966403e-02, -2.50324845e-01,\n",
       "       -3.79605711e-01,  1.25154614e-01,  7.80583143e-01,  1.61494613e-01,\n",
       "        4.15820897e-01,  2.45095193e-01,  4.00414318e-01, -1.95017889e-01,\n",
       "       -4.44737583e-01, -2.68840790e-01, -6.37914300e-01, -7.21030384e-02,\n",
       "       -5.38399935e-01,  9.41851854e-01, -3.30772519e-01,  1.24373935e-01,\n",
       "       -1.27084963e-02, -6.92509592e-01, -1.73538283e-01,  3.14948976e-01,\n",
       "        1.67705104e-01,  7.78147757e-01, -2.25185513e-01, -1.39722019e-01,\n",
       "       -3.05029929e-01,  5.22486746e-01,  2.68080205e-01, -4.30592954e-01,\n",
       "        6.78962409e-01, -3.61038178e-01,  8.15901309e-02,  1.24170512e-01,\n",
       "        3.54962409e-01,  1.51309162e-01,  1.57069102e-01, -6.62496448e-01,\n",
       "       -1.30305424e-01, -3.00738633e-01, -8.82492661e-01, -2.38562107e-01,\n",
       "       -7.87592083e-02,  2.98125803e-01,  4.07160610e-01, -1.99609563e-01,\n",
       "        4.16650593e-01,  1.62382964e-02, -2.48121038e-01,  4.74567026e-01,\n",
       "        4.99644071e-01, -4.68567312e-01,  1.73006669e-01,  1.07625321e-01,\n",
       "        7.81365931e-02,  5.83751976e-01,  1.48482770e-01, -4.79307681e-01,\n",
       "        2.81728119e-01,  1.99407116e-01, -3.99362504e-01, -7.32752740e-01,\n",
       "        8.14741775e-02, -7.61934295e-02,  5.86225271e-01,  8.43391657e-01,\n",
       "       -2.39527032e-01,  3.16045344e-01,  3.89016271e-01,  5.49141586e-01,\n",
       "        3.67586941e-01,  4.53062981e-01,  8.23647082e-01,  1.12830615e+00,\n",
       "       -1.12594470e-01,  5.49060345e-01, -4.72121537e-01, -3.91323894e-01,\n",
       "       -2.05668416e-02, -2.43702501e-01, -5.03571570e-01,  3.26099545e-01,\n",
       "        3.24755728e-01, -4.60193813e-01,  6.54964507e-01, -2.38895506e-01,\n",
       "        1.01198506e+00,  3.79287481e-01, -1.41363516e-01,  1.04771352e+00,\n",
       "       -1.52580813e-01, -5.24565876e-01,  7.39134252e-01, -3.68188351e-01,\n",
       "        4.08782512e-01, -1.63319577e-02,  7.85911858e-01,  3.78897339e-01,\n",
       "       -2.46804610e-01,  5.26438430e-02,  2.10437045e-01,  1.80712342e-01,\n",
       "        4.37517583e-01,  9.89559516e-02,  2.48853460e-01,  1.27546198e-04,\n",
       "       -2.89981961e-01, -3.04868072e-01,  9.03063476e-01, -6.01831794e-01,\n",
       "       -2.21873075e-01,  5.36200643e-01, -1.96828082e-01, -3.74124825e-01,\n",
       "       -4.01867479e-01, -2.20223203e-01,  3.55112672e-01, -4.86021787e-01,\n",
       "       -6.63642347e-01,  2.66324252e-01,  4.16833431e-01, -1.20007612e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the vector representation of any word from our corpus\n",
    "model_w2v['food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v['food'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use the below function to create a vector for each tweet by taking the average of the vectors of the words present in the tweet.\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary                                     \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 200)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparing word2vec feature set…\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will train diffrent models for datasets\n",
    "#first of all we are going to train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will first try to fit the logistic regression model on the Bag-of_Words (BoW) features.\n",
    "# Extracting train and test BoW features \n",
    "train_bow = bow[:31962,:] \n",
    "test_bow = bow[31962:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg = LogisticRegression() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model \n",
    "lreg.fit(xtrain_bow, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lreg.predict_proba(xvalid_bow)\n",
    "# predicting on the validation set \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "# if prediction is greater than or equal to 0.3 than 1 else 0 prediction_int = prediction_int.astype(np.int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.525963149078727"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(yvalid, prediction_int) \n",
    "# calculating f1 score for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(test_bow) \n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int) \n",
    "test['label'] = test_pred_int \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_lreg_bow.csv', index=False) \n",
    "# writing data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Features\n",
    "\n",
    "#We’ll follow the same steps as above, but now for the TF-IDF feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = tfidf[:31962,:] \n",
    "test_tfidf = tfidf[31962:,:] \n",
    "xtrain_tfidf = train_tfidf[ytrain.index] \n",
    "xvalid_tfidf = train_tfidf[yvalid.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5314437555358724"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(xtrain_tfidf, ytrain) \n",
    "prediction = lreg.predict_proba(xvalid_tfidf) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int) \n",
    "# calculating f1 score for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec Features\n",
    "train_w2v = wordvec_df.iloc[:31962,:]\n",
    "test_w2v = wordvec_df.iloc[31962:,:] \n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:] \n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(xtrain_w2v, ytrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6190125276344878"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = lreg.predict_proba(xvalid_w2v) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will implement SVM on our data using the scikit-learn library.\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49906890130353815"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag-of-Words Features\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_bow) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again let’s make predictions for the test dataset and create another submission file.\n",
    "test_pred = svc.predict_proba(test_bow) \n",
    "test_pred_int = test_pred[:,1] >= 0.3 \n",
    "test_pred_int = test_pred_int.astype(np.int) \n",
    "test['label'] = test_pred_int \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_svm_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here validation score is slightly lesser than the Logistic Regression scores for bag-of-words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5104831358249772"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features\n",
    "svc = svm.SVC(kernel='linear', \n",
    "C=1, probability=True).fit(xtrain_tfidf, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_tfidf) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6231546231546231"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Features\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_w2v) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.539607843137255"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag-of-Words Features\n",
    "#First we will train our RandomForest model on the Bag-of-Words features and check its performance \n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain) \n",
    "prediction = rf.predict(xvalid_bow) \n",
    "# validation score \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s make predictions for the test dataset and create another submission file.\n",
    "test_pred = rf.predict(test_bow) \n",
    "test['label'] = test_pred \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_rf_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5521023765996343"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain) \n",
    "prediction = rf.predict(xvalid_tfidf) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5191489361702128"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain) \n",
    "prediction = rf.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\programdata\\anaconda3\\lib\\site-packages (0.90)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5033751205400192"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag-of-Words Features\n",
    "\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain) \n",
    "prediction = xgb_model.predict(xvalid_bow) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5213270142180094"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features\n",
    "\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain) \n",
    "prediction = xgb.predict(xvalid_tfidf) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.642791551882461"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Features\n",
    "\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain) \n",
    "prediction = xgb.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost model on word2vec features has outperformed all the previuos models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FineTuning XGBoost + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "#Here we will use DMatrices. A DMatrix can contain both the features and the target.\n",
    "dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain) \n",
    "dvalid = xgb.DMatrix(xvalid_w2v, label=yvalid) \n",
    "dtest = xgb.DMatrix(test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters that we are going to tune \n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will prepare a custom evaluation metric to calculate F1 score.\n",
    "\n",
    "def custom_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label().astype(np.int)\n",
    "    preds = (preds >= 0.3).astype(np.int)\n",
    "    return [('f1_score', f1_score(labels, preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost with Word2Vec model has given us the best performance so far. Let’s try to tune it further to extract as much from it as we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will use DMatrices. A DMatrix can contain both the features and the target.\n",
    "dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain) \n",
    "dvalid = xgb.DMatrix(xvalid_w2v, label=yvalid) \n",
    "dtest = xgb.DMatrix(test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters that we are going to tune \n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will prepare a custom evaluation metric to calculate F1 score.\n",
    "\n",
    "def custom_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label().astype(np.int)\n",
    "    preds = (preds >= 0.3).astype(np.int)\n",
    "    return [('f1_score', f1_score(labels, preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Approach for Parameter Tuning\n",
    "\n",
    "#We will follow the steps below to tune the parameters.\n",
    "\n",
    "#Choose a relatively high learning rate. Usually a learning rate of 0.3 is used at this stage.\n",
    "\n",
    "#Tune tree-specific parameters such as max_depth, min_child_weight, subsample, colsample_bytree keeping the learning rate fixed.\n",
    "\n",
    "#Tune the learning rate.\n",
    "\n",
    "#Finally tune gamma to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(6,10)\n",
    "     for min_child_weight in range(5,8)\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=6, min_child_weight=5\n",
      "CV with max_depth=6, min_child_weight=6\n",
      "CV with max_depth=6, min_child_weight=7\n",
      "CV with max_depth=7, min_child_weight=5\n",
      "CV with max_depth=7, min_child_weight=6\n",
      "CV with max_depth=7, min_child_weight=7\n",
      "CV with max_depth=8, min_child_weight=5\n",
      "CV with max_depth=8, min_child_weight=6\n",
      "CV with max_depth=8, min_child_weight=7\n",
      "CV with max_depth=9, min_child_weight=5\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tF1 Score 0.6765106 for 54 rounds\n",
      "Best params: 9, 7, F1 Score: 0.6765106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n"
     ]
    }
   ],
   "source": [
    "max_f1 = 0. # initializing with 0 \n",
    "best_params = None \n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "# Cross-validation\n",
    "    cv_results = xgb.cv(        params,\n",
    "        dtrain,        feval= custom_eval,\n",
    "        num_boost_round=200,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=10\n",
    "    )     \n",
    "# Finding best F1 Score\n",
    "mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    \n",
    "boost_rounds = cv_results['test-f1_score-mean'].argmax()    \n",
    "print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))    \n",
    "if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = (max_depth,min_child_weight) \n",
    "\n",
    "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating max_depth and min_child_weight parameters.\n",
    "\n",
    "params['max_depth'] = 8 \n",
    "params['min_child_weight'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning subsample and colsample\n",
    "\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(5,10)]\n",
    "    for colsample in [i/10. for i in range(5,10)] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=0.5, colsample=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tF1 Score 0.6612004 for 46 rounds\n",
      "CV with subsample=0.5, colsample=0.6\n",
      "\tF1 Score 0.6612004 for 46 rounds\n",
      "CV with subsample=0.5, colsample=0.7\n",
      "\tF1 Score 0.6612004 for 46 rounds\n",
      "CV with subsample=0.5, colsample=0.8\n",
      "\tF1 Score 0.6612004 for 46 rounds\n",
      "CV with subsample=0.5, colsample=0.9\n",
      "\tF1 Score 0.6612004 for 46 rounds\n",
      "CV with subsample=0.6, colsample=0.5\n",
      "\tF1 Score 0.6741775999999999 for 58 rounds\n",
      "CV with subsample=0.6, colsample=0.6\n",
      "\tF1 Score 0.6741775999999999 for 58 rounds\n",
      "CV with subsample=0.6, colsample=0.7\n",
      "\tF1 Score 0.6741775999999999 for 58 rounds\n",
      "CV with subsample=0.6, colsample=0.8\n",
      "\tF1 Score 0.6741775999999999 for 58 rounds\n",
      "CV with subsample=0.6, colsample=0.9\n",
      "\tF1 Score 0.6741775999999999 for 58 rounds\n",
      "CV with subsample=0.7, colsample=0.5\n",
      "\tF1 Score 0.6701276 for 58 rounds\n",
      "CV with subsample=0.7, colsample=0.6\n",
      "\tF1 Score 0.6701276 for 58 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tF1 Score 0.6701276 for 58 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "\tF1 Score 0.6701276 for 58 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tF1 Score 0.6701276 for 58 rounds\n",
      "CV with subsample=0.8, colsample=0.5\n",
      "\tF1 Score 0.6640710000000001 for 36 rounds\n",
      "CV with subsample=0.8, colsample=0.6\n",
      "\tF1 Score 0.6640710000000001 for 36 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tF1 Score 0.6640710000000001 for 36 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "\tF1 Score 0.6640710000000001 for 36 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tF1 Score 0.6640710000000001 for 36 rounds\n",
      "CV with subsample=0.9, colsample=0.5\n",
      "\tF1 Score 0.6701788000000001 for 41 rounds\n",
      "CV with subsample=0.9, colsample=0.6\n",
      "\tF1 Score 0.6701788000000001 for 41 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "\tF1 Score 0.6701788000000001 for 41 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tF1 Score 0.6701788000000001 for 41 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "\tF1 Score 0.6701788000000001 for 41 rounds\n",
      "Best params: 0.6, 0.5, F1 Score: 0.6741775999999999\n"
     ]
    }
   ],
   "source": [
    "max_f1 = 0. \n",
    "best_params = None \n",
    "for subsample, colsample in gridsearch_params:\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "     # Update our parameters\n",
    "    params['colsample'] = colsample\n",
    "    params['subsample'] = subsample\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        feval= custom_eval,\n",
    "        num_boost_round=200,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "     # Finding best F1 Score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = (subsample, colsample) \n",
    "\n",
    "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating subsample and colsample_bytree\n",
    "\n",
    "params['subsample'] = .9 \n",
    "params['colsample_bytree'] = .5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tF1 Score 0.680669 for 58 rounds\n",
      "CV with eta=0.2\n",
      "\tF1 Score 0.6907941999999999 for 132 rounds\n",
      "CV with eta=0.1\n",
      "\tF1 Score 0.688855 for 107 rounds\n",
      "CV with eta=0.05\n",
      "\tF1 Score 0.6927606 for 218 rounds\n",
      "CV with eta=0.01\n",
      "\tF1 Score 0.1302024 for 0 rounds\n",
      "CV with eta=0.005\n",
      "\tF1 Score 0.1302024 for 0 rounds\n",
      "Best params: 0.05, F1 Score: 0.6927606\n"
     ]
    }
   ],
   "source": [
    "#Now let’s tune the learning rate.\n",
    "\n",
    "max_f1 = 0. \n",
    "best_params = None \n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "     # Update ETA\n",
    "    params['eta'] = eta\n",
    "\n",
    "     # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        feval= custom_eval,\n",
    "        num_boost_round=1000,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=20\n",
    "    )\n",
    "\n",
    "     # Finding best F1 Score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = eta \n",
    "print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eta'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample': 0.9,\n",
       " 'colsample_bytree': 0.5,\n",
       " 'eta': 0.1,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 6,\n",
       " 'objective': 'binary:logistic',\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s have a look at the final list of tuned parameters.\n",
    "\n",
    "params\n",
    "{'colsample': 0.9,\n",
    " 'colsample_bytree': 0.5, 'eta': 0.1,\n",
    " 'max_depth': 8, 'min_child_weight': 6,\n",
    " 'objective': 'binary:logistic',\n",
    " 'subsample': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally we can now use these tuned parameters in our xgboost model. We have used early stopping of 10 which means if the model’s performance doesn’t improve under 10 rounds, then the model training will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tValidation-error:0.06351\tValidation-f1_score:0.133165\n",
      "Multiple eval metrics have been passed: 'Validation-f1_score' will be used for early stopping.\n",
      "\n",
      "Will train until Validation-f1_score hasn't improved in 10 rounds.\n",
      "[1]\tValidation-error:0.05475\tValidation-f1_score:0.133165\n",
      "[2]\tValidation-error:0.054125\tValidation-f1_score:0.133165\n",
      "[3]\tValidation-error:0.054229\tValidation-f1_score:0.133165\n",
      "[4]\tValidation-error:0.053707\tValidation-f1_score:0.133165\n",
      "[5]\tValidation-error:0.053603\tValidation-f1_score:0.383376\n",
      "[6]\tValidation-error:0.054333\tValidation-f1_score:0.482263\n",
      "[7]\tValidation-error:0.053395\tValidation-f1_score:0.545553\n",
      "[8]\tValidation-error:0.053395\tValidation-f1_score:0.572304\n",
      "[9]\tValidation-error:0.053082\tValidation-f1_score:0.584595\n",
      "[10]\tValidation-error:0.05256\tValidation-f1_score:0.599723\n",
      "[11]\tValidation-error:0.051935\tValidation-f1_score:0.591851\n",
      "[12]\tValidation-error:0.052247\tValidation-f1_score:0.600733\n",
      "[13]\tValidation-error:0.051726\tValidation-f1_score:0.604088\n",
      "[14]\tValidation-error:0.050683\tValidation-f1_score:0.599542\n",
      "[15]\tValidation-error:0.05037\tValidation-f1_score:0.610898\n",
      "[16]\tValidation-error:0.050475\tValidation-f1_score:0.606532\n",
      "[17]\tValidation-error:0.050057\tValidation-f1_score:0.603922\n",
      "[18]\tValidation-error:0.04964\tValidation-f1_score:0.60966\n",
      "[19]\tValidation-error:0.050057\tValidation-f1_score:0.611952\n",
      "[20]\tValidation-error:0.049953\tValidation-f1_score:0.614148\n",
      "[21]\tValidation-error:0.050162\tValidation-f1_score:0.613893\n",
      "[22]\tValidation-error:0.049849\tValidation-f1_score:0.616383\n",
      "[23]\tValidation-error:0.048806\tValidation-f1_score:0.613878\n",
      "[24]\tValidation-error:0.048702\tValidation-f1_score:0.616013\n",
      "[25]\tValidation-error:0.048389\tValidation-f1_score:0.617527\n",
      "[26]\tValidation-error:0.048702\tValidation-f1_score:0.612346\n",
      "[27]\tValidation-error:0.04818\tValidation-f1_score:0.61285\n",
      "[28]\tValidation-error:0.047972\tValidation-f1_score:0.626743\n",
      "[29]\tValidation-error:0.048076\tValidation-f1_score:0.616145\n",
      "[30]\tValidation-error:0.047763\tValidation-f1_score:0.621599\n",
      "[31]\tValidation-error:0.047659\tValidation-f1_score:0.623656\n",
      "[32]\tValidation-error:0.047659\tValidation-f1_score:0.62314\n",
      "[33]\tValidation-error:0.047242\tValidation-f1_score:0.621262\n",
      "[34]\tValidation-error:0.047137\tValidation-f1_score:0.626346\n",
      "[35]\tValidation-error:0.04745\tValidation-f1_score:0.626546\n",
      "[36]\tValidation-error:0.047137\tValidation-f1_score:0.62531\n",
      "[37]\tValidation-error:0.047033\tValidation-f1_score:0.627581\n",
      "[38]\tValidation-error:0.047242\tValidation-f1_score:0.627773\n",
      "[39]\tValidation-error:0.046407\tValidation-f1_score:0.634347\n",
      "[40]\tValidation-error:0.046616\tValidation-f1_score:0.632099\n",
      "[41]\tValidation-error:0.046407\tValidation-f1_score:0.634347\n",
      "[42]\tValidation-error:0.04672\tValidation-f1_score:0.633581\n",
      "[43]\tValidation-error:0.046303\tValidation-f1_score:0.633581\n",
      "[44]\tValidation-error:0.046303\tValidation-f1_score:0.63479\n",
      "[45]\tValidation-error:0.04599\tValidation-f1_score:0.640857\n",
      "[46]\tValidation-error:0.045364\tValidation-f1_score:0.641914\n",
      "[47]\tValidation-error:0.045364\tValidation-f1_score:0.640199\n",
      "[48]\tValidation-error:0.045364\tValidation-f1_score:0.643092\n",
      "[49]\tValidation-error:0.045677\tValidation-f1_score:0.640329\n",
      "[50]\tValidation-error:0.046199\tValidation-f1_score:0.638158\n",
      "[51]\tValidation-error:0.045677\tValidation-f1_score:0.636587\n",
      "[52]\tValidation-error:0.045573\tValidation-f1_score:0.637634\n",
      "[53]\tValidation-error:0.045364\tValidation-f1_score:0.641447\n",
      "[54]\tValidation-error:0.045156\tValidation-f1_score:0.639209\n",
      "[55]\tValidation-error:0.044843\tValidation-f1_score:0.642564\n",
      "[56]\tValidation-error:0.044947\tValidation-f1_score:0.642623\n",
      "[57]\tValidation-error:0.045156\tValidation-f1_score:0.643092\n",
      "[58]\tValidation-error:0.044947\tValidation-f1_score:0.644791\n",
      "[59]\tValidation-error:0.044947\tValidation-f1_score:0.644845\n",
      "[60]\tValidation-error:0.045364\tValidation-f1_score:0.644262\n",
      "[61]\tValidation-error:0.045052\tValidation-f1_score:0.647635\n",
      "[62]\tValidation-error:0.044739\tValidation-f1_score:0.649796\n",
      "[63]\tValidation-error:0.044322\tValidation-f1_score:0.651429\n",
      "[64]\tValidation-error:0.044113\tValidation-f1_score:0.653595\n",
      "[65]\tValidation-error:0.044217\tValidation-f1_score:0.653061\n",
      "[66]\tValidation-error:0.044634\tValidation-f1_score:0.655791\n",
      "[67]\tValidation-error:0.044739\tValidation-f1_score:0.657377\n",
      "[68]\tValidation-error:0.04453\tValidation-f1_score:0.657959\n",
      "[69]\tValidation-error:0.044009\tValidation-f1_score:0.657938\n",
      "[70]\tValidation-error:0.044113\tValidation-f1_score:0.658497\n",
      "[71]\tValidation-error:0.0438\tValidation-f1_score:0.66067\n",
      "[72]\tValidation-error:0.044009\tValidation-f1_score:0.663954\n",
      "[73]\tValidation-error:0.043904\tValidation-f1_score:0.666124\n",
      "[74]\tValidation-error:0.043904\tValidation-f1_score:0.666667\n",
      "[75]\tValidation-error:0.043904\tValidation-f1_score:0.666667\n",
      "[76]\tValidation-error:0.044009\tValidation-f1_score:0.665582\n",
      "[77]\tValidation-error:0.0438\tValidation-f1_score:0.66449\n",
      "[78]\tValidation-error:0.044217\tValidation-f1_score:0.666667\n",
      "[79]\tValidation-error:0.0438\tValidation-f1_score:0.6645\n",
      "[80]\tValidation-error:0.043696\tValidation-f1_score:0.665037\n",
      "[81]\tValidation-error:0.043592\tValidation-f1_score:0.663961\n",
      "[82]\tValidation-error:0.043904\tValidation-f1_score:0.665582\n",
      "[83]\tValidation-error:0.0438\tValidation-f1_score:0.668293\n",
      "[84]\tValidation-error:0.043696\tValidation-f1_score:0.663961\n",
      "[85]\tValidation-error:0.043487\tValidation-f1_score:0.663961\n",
      "[86]\tValidation-error:0.043383\tValidation-f1_score:0.665587\n",
      "[87]\tValidation-error:0.043279\tValidation-f1_score:0.668289\n",
      "[88]\tValidation-error:0.043279\tValidation-f1_score:0.668826\n",
      "[89]\tValidation-error:0.043279\tValidation-f1_score:0.666125\n",
      "[90]\tValidation-error:0.043279\tValidation-f1_score:0.668831\n",
      "[91]\tValidation-error:0.043174\tValidation-f1_score:0.669911\n",
      "[92]\tValidation-error:0.043174\tValidation-f1_score:0.666124\n",
      "[93]\tValidation-error:0.043383\tValidation-f1_score:0.665041\n",
      "[94]\tValidation-error:0.043174\tValidation-f1_score:0.665587\n",
      "[95]\tValidation-error:0.042653\tValidation-f1_score:0.666667\n",
      "[96]\tValidation-error:0.04307\tValidation-f1_score:0.665045\n",
      "[97]\tValidation-error:0.042862\tValidation-f1_score:0.665045\n",
      "[98]\tValidation-error:0.042757\tValidation-f1_score:0.665584\n",
      "[99]\tValidation-error:0.042757\tValidation-f1_score:0.668285\n",
      "[100]\tValidation-error:0.042549\tValidation-f1_score:0.666126\n",
      "[101]\tValidation-error:0.042653\tValidation-f1_score:0.666126\n",
      "Stopping. Best iteration:\n",
      "[91]\tValidation-error:0.043174\tValidation-f1_score:0.669911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    feval= custom_eval,\n",
    "    num_boost_round= 1000,\n",
    "    maximize=True,\n",
    "    evals=[(dvalid, \"Validation\")],\n",
    "    early_stopping_rounds=10\n",
    " )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
